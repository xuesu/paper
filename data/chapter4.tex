\chapter{基于神经网络的情感分类模型}\thispagestyle{fancy}
\section{深度学习相关算法}
本节将从人工神经网络(Artificial Neural Networks, ANN)开始，分别简单介绍反向传播网络(Back Propagation, BP)，卷积神经网络(Convolutional Neural Networks, CNN)，循环神经网络(Recurrent Neural Networks, RNN)，长短时记忆型循环神经网络(Long Short Term Memory, LSTM)，优化算法等模型中可能用到的相关算法。
\subsection{人工神经网络}
人工神经网络提供了一种普遍且实用的方法从样例中学习值为实数，离散值或向量。ANN在一定程度上受到生物大脑中神经元相互连接的模型的启发，它由许多基本单元构成，每个单元有一定的实值输入（该输入可能来自外部也可能是其它单元的输出），同时产生单一的实数值输出。常见ANN的基本单元如图\ref{ann1}:
\begin{figure}[!hbp]
\begin{center}
\includegraphics[width=0.4\textwidth]{graphic/ann1.png}
\caption{ANN基本单元\cite{ml2006} \label{ann1}}
\end{center}
\end{figure}


其函数可以写作：


\begin{equation}
o(\vec{x}) = f(\vec{w} \vec{x} - b)
\end{equation}


其中$\vec{x}$为输入，$f$为激活函数，负责将线性输出转变为非线性输出以增加神经网络的表达能力，$\vec{w}$为权重(weights)，$b$为偏置(bias)，与$\vec{w} \vec{x} $具有相同形状。$\vec{w}$与$b$通常是神经网络训练的主要对象。


常用的激活函数有sigmoid(又称logistic函数),tanh,ReLu(Rectified Linear Unit)等，具体公式如下：


\begin{center}
\begin{tabu}  to 0.8\textwidth{X|X[3]|X[3]}
\hline
函数名称 & 函数 & 导数 \\
\hline
sigmoid &
$$
f(y) = \frac{1}{1 + e^{-y}}
$$
&
$$
\frac{df(y)}{dy} = f(y) \times (1 - f(y))
$$
\\ \hline
tanh &
$$
f(y) = tanh(y) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$
&
$$
\frac{df(y)}{dy} = 1 - {f(y)}^2
$$
\\ \hline
ReLU &
$$
f(y) = \left\{\begin{matrix}
y & if\ y > 0\\ 
0 & if\ y \leq 0
\end{matrix}\right.
$$
&
$$
\frac{df(y)}{y} = \left\{\begin{matrix}
1 & if\ y > 0\\ 
0 & if\ y \leq 0
\end{matrix}\right.
$$
\\ \hline
\end{tabu}
\end{center}
由于sigmoid函数和tan函数的导数是线性输出的二次多项式，在训练时易导致梯度爆炸或梯度消失问题，故本文中线性层之间的激活函数主要选用ReLU函数。\par
一个较典型的人工神经网络通常由多层基本单元构成，如图\ref{ann2}，通常有输入层，隐含层，输出层，各层之间通过激活函数将线性输出转化为非线性输出。\par


\begin{figure}[!hbp]
\begin{center}
\includegraphics[width=0.4\textwidth]{graphic/ann2.png}
\caption{典型ANN结构\cite{ml2006} \label{ann2}}
\end{center}
\end{figure}

\subsection{损失计算函数}
本文中主要使用带有softmax的交叉熵函数计算网络输出值和目标值之间的误差。softmax函数是sigmoid函数在多分类问题上的拓展。


\begin{equation}
softmax(t_i) = \frac{e^{t_i}}{\sum_j{e^{t_j}}}
\end{equation}


其中$\vec{t}$为网络的输出，其长度为类别总数。


\begin{equation}
loss = crossentropy(\vec{S}, \vec{L}) 
= \sum_i {(L_i * log(s_i))}, \vec{S} = softmax{(\vec{t})}
\end{equation}


其中$\vec{L}$为网络的目标值，其长度为类别总数。

\subsection{反向传播网络}
反向传播算法是目前多层神经网络主要的训练方法，它主要基于微积分的链式求导法则\ref{chainrule}，对神经网络从输出层向上进行快速求导。

\begin{equation} \label{chainrule}
(f(g(y)))' = f'(g(y))g'(y)
\end{equation}
反向传播神经网络的训练基本流程是：\par
\begin{enumerate}
\item 初始化网络。
\item 首先将输入沿前向传播，求出误差。
\item 使误差沿网络反向传播，计算梯度，从而使用优化方法更新权值。
\item 如果达到终止条件，跳出循环，否则，重复第2-4步。
\end{enumerate}

\subsection{优化方法}
在神经网络上训练时，其假设空间为所有可能的实数权向量的集合（将偏置等参数也视为权向量），一个高维空间。当误差函数和目标值已知时，假设空间中会形成一个误差曲面，而训练的目的就是尽可能找到该误差曲面的最小值。\par
如图\ref{ann3}是二维假设空间中误差曲面的假想图，可以看出该误差曲面是具有单一全局最小值的抛物面，沿梯度方向可以到达该最小值点。\par
\begin{figure}[!hbp]
\begin{center}
\includegraphics[width=0.4\textwidth]{graphic/ann3.png}
\caption{误差曲面\cite{ml2006} \label{ann3}}
\end{center}
\end{figure}
\subsubsection{随机梯度下降方法}
为了确定一个使误差最小化的权向量，梯度下降搜索从初始权向量出发，每一步都沿梯度方向修改该权向量，直到到达全局最小误差点。\par
该训练法则可以写为：\cite{ml2006}
\begin{equation}
w_i \leftarrow w_i + \eta\Delta w_i
\end{equation}


其中$\Delta w_i$为误差相对于分量$w_i$方向上的梯度，$\eta$为控制步长的学习速率。\par
但梯度下降方法每一步都需要计算所有训练样例上的整体误差，同时，梯度下降方法很可能收敛到局部极小值。\par
为了缓解这些困难，人们提出了随机梯度下降算法，该算法根据每个训练样例batch单独计算误差来更新权值，相当于为每个batch单独定义不同的误差函数。因此，如果误差平面上有多个局部极小值，随机梯度下降算法可以通过不同的误差避免陷入局部最小值。
\subsubsection{Adagrad方法}
虽然随机梯度下降方法为避免陷入局部最小值提供了可行的方法，但梯度下降方法对所有需要更新的权向量都使用了全局学习速率，而全局学习速率可能不适应于所有的参数。Adagrad(Adaptive Subgradient)方法\cite{adagrad}能够对每个参数自适应不同的学习速率，对稀疏特征使用更大的学习速率，因此更适应于处理稀疏数据。\par


其学习速率为：


\begin{equation}
\eta_{t, i} = \frac{\eta_0}{ \sqrt{\sum_{j=1}^{t}{G_{j,i}^2} + \epsilon}}
\end{equation}


其中,$\eta_{t, i}$为t时刻对于权重$w_i$的学习速率，$\eta_0$为初始学习速率，$\epsilon$为平滑常数，用于防止分母为0，$G_{j,i}$代表j时刻$w_i$方向上的梯度。

\subsubsection{ADADELTA方法}
Zeiler等\cite{adadelta}人认为Adagrad方法存在三个问题:


\begin{enumerate}
\item 其学习率单调递减，训练后期学习率过小。
\item 需要手工设置一个全局的初始学习率。
\item 更新$X_t$时存在单位不统一现象。
\end{enumerate}


因此Zeiler等人提出了ADADELTA方法，用于改进Adagrad方法。ADADELTA算法基于牛顿迭代法，其学习速率不再基于全部梯度平方之和，而主要基于最近的梯度。


其学习速率更新规则为：
\begin{equation}
E[g^2]_t = \rho E[g^2]_{t-1} + (1-\rho )g_t^2
\end{equation}
\begin{equation}
\eta_t =  -\frac{\sqrt{E[\eta^2]_{t-1} + \epsilon}}{\sqrt{E[g^2]_{t} + \epsilon}}g_t
\end{equation}
\begin{equation}
E[\eta^2]_t = \rho E[\eta^2]_{t-1} + (1-\rho )\eta_t^2
\end{equation}
其中$g_t$为t时刻的梯度，$\rho$为衰减速率，$\epsilon$为平滑常数。

\subsubsection{ADAM方法}
ADAM(Adaptive Moment Estimation)\cite{adam}根据损失函数对每个参数的梯度的一阶矩估计和二阶矩估计动态调整针对于每个参数的学习速率，其中一阶矩的作用类似于冲量项(momentum)，二阶矩则与ADADELTA算法相似。ADAM算法迭代更新步长有一个较为稳定的范围，因此更适合RNN。


\begin{equation}
m_t = \beta_1 m_{t - 1} + (1 - \beta_1)g_t
\end{equation}
\begin{equation}
v_t = \beta_2 m_{t - 1} + (1 - \beta_2)g_t^2
\end{equation}
\begin{equation}
\hat{m_t} = \frac{m_t}{1 - \beta_1^t}
\end{equation}
\begin{equation}
\hat{v_t} = \frac{v_t}{1 - \beta_2^t}
\end{equation}
\begin{equation}
\eta_t = -\frac{\eta}{\sqrt{\hat{v_t} + \epsilon}} \hat{m_t}
\end{equation}


实践中$\beta_1=0.9$，$\beta_2=0.999$，$\epsilon = 1e-8$

\subsection{卷积神经网络}
卷积神经网络(Convolutional Neural Network, CNN)是一种前馈网络，该网络内部单元的连接模式不同于传统的全连接方式，每个神经元的输入仅与上一层对应单元核大小范围内的输出单元有关。卷积神经网络相当于在神经网络上执行卷积操作。\par
数学上的卷积函数定义如下：\par
设$x(a)$和$w(a)$为在$R$上的可导函数，则称$s(t) = \int x(a)w(t-a)da$为函数$x$与$w$的卷积，记作$s(t) = (x * w)(t)$\par
其中$x$被称为输入，$w$被称为核函数(kernel function)，在统计学上，卷积操作相当于加权平均。\par
由于自然语言为离散序列，故本文使用的卷积运算核为1维，卷积公式可以写为：\par
\begin{equation}
s(t) = (x * w)(t) = \sum x(a)w(t -a)
\end{equation}


在实际使用中，a的取值范围通常较小，以减少运算，产生局部输出单元的综合输出。


下采样(downsampling)卷积函数能够每隔一段距离采样，称该距离为步长(stride)，则当核为1维，核长为k时，卷积公式可以写为：
\begin{equation}
s(t) = (x * w)(t) = \sum_{i = 0}^{\lfloor{\frac{k}{stride}}\rfloor}{x(t + i \times stride)w(i \times stide)}
\end{equation}


卷积操作通过三个重要的思想帮助改进机器学习系统，即稀疏交互，参数共享和平移不变性\cite{deeplearning2016}。首先是稀疏交互，如图\ref{cnn1}，传统神经网络单元会与上一层的每个输入单元产生交互，而通过限制卷积核大小，单元只会与附近的输入单元产生交互，从而大大减少了需要训练和存储的参数，提升了效率和泛化能力。然后是参数共享，在卷积神经网络中，多个位置共享同样的参数集合，而不需要针对每个位置单独训练。最后是平移不变性。如果我们在输入中移动一个事件，那么对应的输出仍会输出而且会延后相应长度。例如，对于“手机 小巧 玲珑”和“我 觉得 手机 小巧 玲珑”，当卷积核的大小为2，步长为2时，两句中的“手机 小巧”将会输出同样的结果。

\begin{figure}[!hbp]
\begin{center}
\includegraphics[width=0.4\textwidth]{graphic/cnn1.png}
\caption{CNN与ANN对比\cite{deeplearning2016} \label{cnn1}}
\end{center}
\end{figure}

\subsubsection{池化}
池化(Pooling)是卷积神经网络中非常重要的操作，能帮助学习输入特征的不变性，也能减少特征数量，增加计算速度。常见的池化方式有两种，最大池化和平均池化。最大池化给出了卷积层输出中一个矩形区域内特征的最大值，而平均池化则为矩形区域内特征的平均值。池化相当于简单使用某一位置附近区域的总体输出来代替各单元的输出。在自然语言处理中，最大池化的作用往往要好于平均池化，因此，本文采用最大池化方式。
\subsection{C\&W模型}
C\&W模型是由collobert等于2011年\cite{collobert2011} 提出的基于卷积神经网络的自然语言处理框架，该模型被设计能够适用于几乎所有自然语言处理任务，同时也能对某个任务进行细化以提高精度。C\&W模型分为两种模式，单词窗口和语句模式，后者主要结构如图\ref{candw1}。模型可以分为七层，输入层，查表层，卷积层，池化层，线性层和对应的激活函数层，最后是输出层。C\&W模型在各任务上都表现良好，接近为特定任务特化的系统性能，因此本文选择C\&W模型作为基本框架。
\begin{figure}
\begin{center}
\includegraphics{graphic/candw1.png}
\caption{C\&W语句模式基本模型 \label{candw1}}
\end{center}
\end{figure}
\subsection{循环神经网络}
循环神经网络(Recurrent Neural Network)是专门用于处理序列数据的神经网络。如图，传统神经网络为序列中的每个状态分别训练权重，但RNN在时间步内共享参数。给定时间t的变量后，t+1时变量的条件概率分布是平稳的，不依赖于t，也即RNN具有时间上的平移不变性。\par
对于长度为{t}的序列$\vec{x}$，一般神经网络计算方式可以表示为：
\begin{equation}
h_t = g_t(x_t, x_{t-1},...,x_0;\theta)
\end{equation}


其中$\theta$代表所有网络参数。\par
而RNN的计算方式可以表示为:


\begin{equation}
h_t = g_t(x_t, x_{t-1},...,x_0;\theta) = f(h_{t-1}, x_t;\theta)
\end{equation}


RNN能将长度为t的序列映射为固定长度的向量，但这种映射通常是有损的，因此，需要保证$h_t$足够丰富，能够记住序列中的信息。


RNN主要有三种结构，a中每个时间步都有输出，并且隐藏单元与过去隐藏单元相关联，b中隐藏单元与过去输出相关联，而c中只有最后一个时间步有输出。由于语句级情感分析任务最终只有一个输出，所以本章中RNN结构符合c。


训练中，RNN使用通过时间反向传播算法(BPTT)反向推导t步梯度。实践中，为了降低复杂度，同时也因为梯度爆炸和梯度消失的限制，t往往取较小的定值。
\begin{center}
\begin{figure}
\subfigure{
	\includegraphics[width=0.3\textwidth]{graphic/rnn1.PNG}
	\label{fig:rnn1}
}
\subfigure{
	\includegraphics[width=0.3\textwidth]{graphic/rnn2.PNG}
	\label{fig:rnn2}
}
\subfigure{
	\includegraphics[width=0.3\textwidth]{graphic/rnn3.PNG}
	\label{fig:rnn3}
}
\caption{RNN基本结构}
\end{figure}
\end{center}

\subsection{LSTM}
RNN为信息的持久化提供了行之有效的方法，但RNN存在梯度爆炸或梯度消失的问题。\par
考虑最简单的RNN单元，即：
\begin{equation}
h_t = W^Th_{t-1}
\end{equation}


该公式相当于：


\begin{equation}
h_t = {W^t}^Th_0
\end{equation}


若W符合下列形式的特征分解：


\begin{equation}
W=Q\Lambda Q^T
\end{equation}


则原式可以转化为：


\begin{equation}
h_t = Q^T\Lambda ^t Qh_0
\end{equation}


因此，$h_t$随时间成指数形式增长，若幅值小于1时，$h_t$将随t的增长趋向于0，出现梯度消失现象，而当幅值大于1时，$h_t$又激增，导致梯度爆炸。\par
为了减轻梯度消失问题，Hochreiter等\cite{lstm1997}提出了LSTM模型，该模型主要由以下四部分构成：


\begin{enumerate}
\item 遗忘门：控制状态更新
\item 输入门：控制信息保存。
\item 输出门：控制读取信息。
\item 记忆单元：存储状态。
\end{enumerate}


LSTM的具体计算流程如下：


\begin{equation}
i_t = \sigma(W_i  [h_{t-1},x_t] + b_i)
\end{equation}


其中$i_t$为输入门输出的结果，$W_i$为输入门的参数，$h_{t-1}$为上一时刻的输出，$x_t$为本时刻的输入，$b_i$为输入门的参数偏置。


\begin{equation}
u_t = \tanh{(W_u [h_{t-1},x_t] + b_u)}
\end{equation}


其中$u_t$为记忆单元得到的结果，$W_u$为记忆单元的参数，$b_u$为输入门的参数偏置。


\begin{equation}
f_t = \sigma{(W_f [h_{t-1},x_t] + b_f)}
\end{equation}


其中$f_t$为遗忘门输出的结果，$W_f$为遗忘门的参数，$b_f$为遗忘门的参数偏置。


\begin{equation}
C_t = f_t C_{t-1} + i_t \times u_t
\end{equation}


其中$C_t$为本时间点的状态。


\begin{equation}
o_t = \sigma{(W_o [h_{t-1},x_t] + b_o)}
\end{equation}


其中$o_t$为输出门输出的结果，$W_o$为输出门的参数，$b_o$为输出门的参数偏置。
\begin{equation}
h_t = o_t \times tanh{(C_t)}
\end{equation}


其中$h_t$为最终输出结果。


\begin{figure}[!hbp]
\begin{center}
\includegraphics[width=0.4\textwidth]{graphic/lstm1.png}
\caption{LSTM基本结构\cite{deeplearning2016} \label{lstm1}}
\end{center}
\end{figure}

\section{NOLSTM模型}
本文基于C\&W模型实现了情感分类模型，由于卷积神经网络具有平移不变性，因此可以抽取出语句中短语特征，从而借此分析整体情感。
\subsection{神经网络结构}
NOLSTM模型主要分为八层，输入层，连接层，卷积层，Dropout层，三层线性层和输出层。\par
其中，如图\ref{nolstm1}输入层和连接层对应于C\& W模型的输入层和查表层，输入的离散特征在输入层转换为one-hot编码或者经过训练的编码，连接层将输入的特征包括连续特征进行连接，拓展每个词所对应的特征向量维数。输入层设计为与具体特征选择无关，以便于调整模型，也便于未来用不同的特征对模型进行拓展。\par
如图\ref{nolstm1}，经过反复试验，本文中卷积层由一维五个卷积核构成，本文中取[3,5,6,7,9]，不同的卷积核分别对应不同的池化层。本文中选取的池化层都为全局池化层，经过全局池化层可以提取出文本在不同长度上的全局特征。令人惊讶的是，同样经过反复试验，全局池化层的效果好于局部池化层或者堆叠局部池化层，笔者认为局部池化可以保留语句结构，而全局池化可以去除语句结构信息，而在神经网络对语句分析的过程中，过于丰富的语句结构可能反而起了抑制泛化的作用，详细参数分析见第五章。\par
Dropout层被加在卷积层之后以便于提高模型泛化性。Dropout能够随机使一些卷积层的输出失效，以迫使模型利用其它输出结果，从而尽量减少特征与某一特定输出之间的关联。\par
模型接下来使用了三层特征数目依次递减的全连接层即Linear0，Linear1，Linear2抽取高级特征，如图\ref{nolstm3}，线性层之间使用ReLU函数进行激活。\par
最后模型进行误差和精度计算，并使用ADAM优化函数即图中的Optimizer进行反向传播。图中的save用于保存和读取模型。\par
由于该模型相对于特征独立，所以该模型可以被同时应用于中英双语，也可以拓展到其它语句分类任务上。


\begin{figure}[!hbp]
\begin{center}
\includegraphics[width=0.8\textwidth]{graphic/nolstm1.png}
\caption{NOLSTM模型整体结构 \label{nolstm1}}
\end{center}
\end{figure}
\begin{figure}[!hbp]
\begin{center}
\includegraphics[width=0.8\textwidth]{graphic/nolstm2.png}
\caption{NOLSTM模型Input层 \label{nolstm2}}
\end{center}
\end{figure}
\begin{figure}[!hbp]
\begin{center}
\includegraphics[width=0.8\textwidth]{graphic/nolstm3.png}
\caption{NOLSTM模型卷积层 \label{nolstm3}}
\end{center}
\end{figure}
\begin{figure}[!hbp]
\begin{center}
\includegraphics[width=0.8\textwidth]{graphic/nolstm4.png}
\caption{NOLSTM模型输出层 \label{nolstm3}}
\end{center}
\end{figure}
\subsection{模型主要流程}


模型分为训练模式与测试模式：\par
在训练模式下，基本步骤为：\par
\begin{enumerate}
\item 数据生成器按不同的语言要求生成输入特征，对超过限定长度的语句直接截断，以语句前半部分生成输入特征，实践证明往往前半部分已经拥有足够判断情感极性的信息。
\item 模型将这些特征通过输入层和连接层转化为统一的向量，计算误差。
\item 通过优化函数Optimizer，将误差反向传播以训练参数。
\item 每隔特定步数，对验证集进行测试，测试时不启动反向传播。
\item 若验证集精度大于已有最好精度，保存该模型，同时在测试集上进行测试，本文认为在测试集上的精度衡量了其泛化能力。
\end{enumerate}


在测试模式下，基本步骤为：


\begin{enumerate}
\item 载入模型。
\item 使用数据生成器将单一语句转化为输入特征。
\item 计算各标签的概率，即Output层中的Logits，输出概率最高的标签。
\end{enumerate}
\subsection{模型主要改进}
相对于C\& W模型，本文根据情感分析任务对模型进行了特化。
\begin{enumerate}
\item 待编码特征通过word2vec进行训练，以加快训练速度。
\item 选择全局池化，而不是C\& W模型中的时延模型，以舍弃语法特征。
\item 直接截断语句，取前半部分计算，以加快计算速度，减少空间损耗。
\end{enumerate}
\section{CLSTM模型}
由于文本是不定长的离散序列，故RNN适应于对文本建模和分析。本文使用卷积神经网络结合LSTM模型实现了基于循环神经网络的情感分类模型，简称为CLSTM模型，以与NOLSTM相区别。
\subsection{神经网络结构}
如图\ref{clstm1}，CLSTM模型主要分为八层，输入层，连接层，卷积层，池化层，Dropout层，第一线性层，LSTM单元层（图\ref{clstm2}），第二线性层。其中输入层，连接层，卷积层，池化层与NOLSTM模型相同。采用与NOLSTM模型相同的卷积层可能违反直觉，因为RNN能够有效利用基于时序的局部特征，但实际上恰恰相反，不使用卷积神经网络或者使用局部池化及堆叠局部池化反而导致模型后期精度下降，这可能同样由语法结构可能反而会导致过拟合这一原因导致的。在实际测试中，该网络结构的性能较为出色。\par
在对输出特征进行dropout之后，特征首先进入第一线性层而不是LSTM层，以减少特征数量，抽取高级特征，便于LSTM层进行保存。最后，模型通过第二线性层以及输出层预测和计算输出结果。总而言之，LSTM层取代了NOLSTM模型中的第二线性层。
\subsection{模型主要流程}
模型分为训练模式与测试模式，设stepNum为LSTM单元迭代次数，在训练模式下，基本步骤为：\par
\begin{enumerate}
\item 数据生成器按不同的语言要求生成stepNum步输入特征。
\item 执行第3-4步直到该batch中的句子全部遍历完毕。
\item 模型将stepNum步特征全部通过输入层和连接层转化为统一的向量，计算误差。
\item 通过优化函数Optimizer，将误差反向传播以训练参数。
\item 每隔特定步数，对验证集进行测试，测试时不启动反向传播。
\item 若验证集精度大于已有最好精度，保存该模型，同时在测试集上进行测试，本文认为在测试集上的精度衡量了其泛化能力。
\end{enumerate}
在测试模式下，基本步骤为：\par
\begin{enumerate}
\item 载入模型。
\item 使用数据生成器将单一语句转化为stepNum步输入特征。
\item 重复执行第3-4步直到该句子被遍历完毕。
\item 计算各标签的概率，即Output层中的Logits，输出概率最高的标签。
\end{enumerate}
\begin{figure}[!hbp]
\begin{center}
\includegraphics[width=0.8\textwidth]{graphic/clstm1.png}
\caption{CLSTM模型整体结构 \label{clstm1}}
\end{center}
\end{figure}
\begin{figure}[!hbp]
\begin{center}
\includegraphics[width=0.4\textwidth]{graphic/clstm2.png}
\caption{CLSTM模型LSTM层 \label{clstm2}}
\end{center}
\end{figure}
\subsection{模型主要优化}
CLSTM模型结合了LSTM模型与卷积神经网络，能够在提取全局短语特征的同时处理不定长序列。相较于NOLSTM模型，它能更好地利用自然语言地序列属性，训练更少但更有效的参数。实践中，CLSTM模型往往比NOLSTM模型取得更好的成果。